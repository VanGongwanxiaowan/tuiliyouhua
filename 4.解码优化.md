<img width="631" height="413" alt="image" src="https://github.com/user-attachments/assets/221d0aad-5841-4667-b743-5f1ae569778f" />


<img width="624" height="324" alt="image" src="https://github.com/user-attachments/assets/bd6e07b1-88c1-40ce-baba-70783f629463" />

### 一、Blockwise Parallel Decoding 相关内容
- **出发点**：贪心解码需逐步产生长度为\( m \)的输出，每次生成一个token都要进行全模型参数激活与查询，过程受限于内存带宽，且模型只能预测下一个token。若用\( k-1 \)个辅助模型，每个模型可根据输入序列预测对应位置的token，辅助模型和原始模型可独立运行，从而并行生成\( k \)个token。
- **三个阶段**：Predict、Verify和Accept阶段。
  - Predict阶段：用\( k-1 \)个辅助模型预测\( k \)个位置token的预测值。
  - Verify阶段：用原模型验证这\( k \)个位置，将其组成batch，实现合适的attention mask，一次性获得\( k \)个位置的概率分布，贪心选择概率最大的token作为验证结果。
  - Accept阶段：若验证后的token和Predict阶段预测的token相同则保留，不同则预测的token预测错误。
- **优化**：
  - 理想情况下（每次触发不预测结果）生成长度\( m \)的序列所需解码次数降低为\( 2m/k \)，进一步优化后在Verify阶段同时预测\( k \)个token，使Predict和Verify阶段合并，验证时也获得\( k \)个token的梯度，将第一次迭代后每次迭代只需调用一次模型forward，模型调用次数减半。
  - 进一步将模型调用次数从\( 2m/k \)减少到\( m/k + 1 \)，只需对数据做偏移修改，无需负载预测后\( k \)个token的模型，在decoder最后一个projection layer前插入一个PNN，输入为batch_size, sequence_length, \( d_{model} \)，输出为batch_size, sequence_length, \( k^2 \), \( d_{model} \)，在decoder输出和project layer输入处进行残差连接，project layer输出\( k \)个不同位置token的logits。因训练时内存限制，无法使用对应project layer输出的交叉熵损失的平均值作为loss，而是为每个minibatch随机均匀选择其中一个layer输出作为loss，训练可通过frozen、finetuning、distillation的任何一种进行。

### 二、采样/解码细分方法
- speculative decoding = draft model + reject sampling + parallel verification
- Blockwise Parallel Decoding = multi-draft model + top-1 sampling + parallel verification
- SpecInfer = Speculative decoding + token tree verification (top-k sampling + parallel verification)
- Medusa = multi-decoding head + tree attention + typical acceptance (threshold)
- LLMA = reuse + parallel verification
- Lookahead Decoding = n-gram + Jacobi iteration + parallel verification
- EAGLE = word embedding + simplified tree verification + multi-run speculative sampling
- SoT = Prompt-based Parallel Decoding

### 三、大模型推理优化策略
- **7.1 显存优化**
- **7.2 算子融合**
- **7.3 高性能算子**
- **7.4 调度优化**
- **7.5 量化**
- **7.6 模型并行**
- **7.7 通信优化**
- **7.8 采样/解码**
- **参考资料**

SpecInfer采用了两项关键技术来提高解码速度：**Speculative Inference**和**Token Tree Verification**。
- **Speculative Inference**：采用一批small speculative models（SSMs），并行预测多个候选。SSM可以是原始LLM的蒸馏、量化、剪枝版本（甚至是可供检索的知识库如LLMA、用户的自定义函数），由于SSM的参数规模通常会小2-3个数量级，能力有限，SpecInfer引入了“collective boost-tuning”技术，基于adaptive boosting的思想，对SSM进行微调，让SSM与原始LLM更紧密地对齐，提升预测准确率，降低Verification的成本。
- **Token Tree Verification**：将SSMs预测的多个候选merge为一个新的token tree，采用原始LLM做并行验证。为了做到高效地并行验证，SpecInfer引入了“Tree-based Parallel Decoding”机制；为了适配通用的随机采样方式，SpecInfer引入了“multi-step speculative sampling”算法。Token Tree Verification使用了“topology-aware masking”的方式来一次性计算整个token-tree的attention（一次性计算整一棵树，在一个kernel里面完成开销更小）；一些sequence其实有共同的prefix，计算一整棵树时，相当于复用这些prefix的KV cache，减少了I/O开销。

<img width="1021" height="766" alt="image" src="https://github.com/user-attachments/assets/9998f2c5-83d7-456f-8a8a-fd0c2b0e8048" />



<img width="978" height="661" alt="image" src="https://github.com/user-attachments/assets/35e1d8ef-2726-40a0-bfc4-903915d7118e" />

### Medusa 相关内容
- **原理**：通过为Transformer大模型添加\( n \)个解码头（decoding heads），每个头部是一个单层前馈网络，使Medusa让大模型一次性并行生成\( n \)个分布，而非逐个生成。若在\( n \)个分布中选取top-k，可得分\( n \times \text{top-k} \)个词，对这\( n \times \text{top-k} \)个词做卡尔可夫大模型提高候选词准确率；Medusa提出结合tree attention，以并行方式验证这些词，从而实现推理加速。具体来说是将每个头的top-k个词作为节点，每个头作为树的一层，每条直到叶子节点的路径构成一组待验证的预测。在这棵树内，Attention Mask需要新的设计，使Mask只限制对一个token的前面token的注意力，同时要为相应地为position embedding设置正确的位置索引。
- **采样方法扩展**：上述方式仍受限于greedy search的解码能力，Medusa提出了Typical acceptance，使之支持top-k、top-p方式的采样方法。其出发点在于：实际使用中为控制模型创造性而改变采样温度，会使随机采样中的draft模型与target模型的分布不一致，从而top-p采样可能会拒绝draft模型生成，导致并行解码长度很短。作者从truncation sampling工作中汲取灵感，旨在选择原始模型可能接受的候选项，根据原始模型的预测概率设定一个阈值，若候选项超过这个阈值，就会被接受；作者采取hard threshold和entropy-dependent threshold的最小值来决定是否像在truncation sampling中那样接受一个候选token，这确保了在解码过程中选择有意义的token和合理的延续。
- **特点**：作者总是使用Greedy Decoding接受第一个token，确保每一步至少生成一个token，最终的输出则是通过acceptance test的最长序列；使用Medusa并不需要重新训练整个大模型，而是冻结大模型而只训练解码头。
- **公式化表述**：Medusa = multi-decoding head + tree attention + typical acceptance(threshold)

### 大模型推理优化策略
- **7.1 显存优化**
- **7.2 算子融合**
- **7.3 高性能算子**
- **7.4 调度优化**
- **7.5 量化**
- **7.6 模型并行**
- **7.7 通信优化**
- **7.8 采样/解码**
  - speculative decoding = draft model + reject sampling + parallel verification
  - Blockwise Parallel Decoding = multi-draft model + top-1 sampling + parallel verification
  - SpecInfer = Speculative decoding + token tree verification(top-k sampling + parallel verification)
  - LLMA = reuse + parallel verification
  - Lookahead Decoding = n-gram + Jacobi iteration + parallel verification
  - EAGLE = word embedding + simplified tree verification + multi-run speculative sampling
  - SoT = Prompt-based Parallel Decoding
- **参考资料**



<img width="899" height="552" alt="image" src="https://github.com/user-attachments/assets/f18535a2-668d-480b-be9f-e4af21ef4041" />


<img width="899" height="624" alt="image" src="https://github.com/user-attachments/assets/39426c5c-bc74-46de-bc79-a31cf4ea99be" />

<img width="1011" height="630" alt="image" src="https://github.com/user-attachments/assets/a8a06260-0045-4610-9c78-7411b834fb6a" />


<img width="1016" height="711" alt="image" src="https://github.com/user-attachments/assets/7d74024c-e0ba-4d5c-b6f8-0129589196cc" />


<img width="1013" height="634" alt="image" src="https://github.com/user-attachments/assets/897ee120-e2f4-4687-89e9-f9fd2f3f7f67" />


<img width="1089" height="699" alt="image" src="https://github.com/user-attachments/assets/ba972928-98e5-41fd-a8da-801f8815499b" />

<img width="989" height="365" alt="image" src="https://github.com/user-attachments/assets/6315f62b-72a4-4447-b2d1-c9b76699d5e4" />




