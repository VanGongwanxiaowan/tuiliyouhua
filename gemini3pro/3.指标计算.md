没问题，我们继续推进！🚀

在上一节，我们知道了模型推理分为 **Prefill（预填充）** 和 **Decode（解码）** 两个阶段。
这一节，我们将学习如何用**数字**来衡量这两个阶段的好坏。

**“不能度量，就无法优化。”** —— 这是工程界的铁律。

在 LLM 推理优化中，最核心的指标主要分为两类：**延迟（Latency）** 和 **吞吐量（Throughput）**。这就像评价一家餐厅：
1.  **延迟**：菜上得快不快？（单客体验）
2.  **吞吐量**：一天能接待多少客人？（系统容量）

---

### **1. 延迟指标 (Latency Metrics) - 关注“快”**

这三个指标直接决定了**单个用户**的使用体验。

#### **(1) 首字延迟 (TTFT - Time To First Token)**
*   **定义**：从用户点击“发送”，到屏幕上蹦出**第一个字**所花的时间。
*   **对应阶段**：主要对应 **Prefill 阶段** 的耗时。
*   **为什么重要**：
    *   这是用户体验的**第一印象**。如果 TTFT 超过 3秒，用户就会觉得“卡了”或者“没反应”。
    *   在流式输出（Streaming）场景下（如 ChatGPT），这是最重要的指标。
*   **优化目标**：通常要求 `< 200ms` (极致流畅) 或 `< 1s` (可接受)。

#### **(2) 每 Token 延迟 (TPOT - Time Per Output Token)**
*   **定义**：第一个字出来后，后续每一个字蹦出来的平均间隔时间。
*   **对应阶段**：对应 **Decode 阶段** 的耗时。
*   **为什么重要**：
    *   决定了生成的“流畅度”。
    *   人类的平均阅读速度大约是 **3-5 Token/秒**。如果你的 TPOT 对应的速度慢于阅读速度，用户就会感到焦躁。
*   **计算公式**：$\text{TPOT} = \frac{\text{Decode 阶段总耗时}}{\text{生成的 Token 总数}}$

#### **(3) 端到端延迟 (End-to-End Latency)**
*   **定义**：从发出请求到收到**完整**回复的总时间。
*   **公式**：$\text{Total Latency} = \text{TTFT} + (\text{TPOT} \times \text{生成长度})$
*   **适用场景**：非流式应用（比如翻译一段话，或者代码补全），用户必须等全部生成完才能看到结果。

---

### **2. 吞吐量指标 (Throughput Metrics) - 关注“多”**

当我们从“服务一个用户”变成“服务一万个用户”时，这个指标就决定了你的**成本**。

#### **(1) Token 吞吐量 (Tokens Per Second - TPS)**
*   **定义**：推理服务在单位时间（如1秒）内，**所有并发请求**一共生成了多少个 Token。
*   **意义**：
    *   这是衡量 GPU 算力利用率的最直接指标。
    *   TPS 越高，说明你的显卡“干的活”越多，“摸鱼”等待的时间越少。
*   **注意**：这里的 Tokens 通常指 **Output Tokens** (生成的词)，有时也会包含 Input Tokens，看具体定义，但通常优化关注 Output TPS。

#### **(2) 请求吞吐量 (Requests Per Second - RPS / QPS)**
*   **定义**：每秒能处理完成多少个完整的用户请求。
*   **意义**：业务层更关心的指标（比如：我的服务器能撑住双十一的流量吗？）。

---

### **3. 核心权衡：延迟 vs. 吞吐量 (The Trade-off)**

这是优化中最反直觉、也是最关键的知识点：
**延迟和吞吐量通常是互斥的。**

*   **场景 A：追求极致低延迟 (Batch Size = 1)**
    *   每次只处理 1 个用户的请求。
    *   **结果**：TTFT 和 TPOT 极低（快），但显卡大部分时间在搬运权重（Memory-bound），利用率低，**吞吐量极低**。
    *   *比喻：法拉利送一粒大米，送得快，但运力浪费。*

*   **场景 B：追求极致吞吐量 (Large Batch Size)**
    *   把 100 个用户的请求攒在一起处理 (Batching)。
    *   **结果**：单次加载权重可以给 100 个人用，GPU 计算单元填满了，**吞吐量极大**。但每个用户的请求都要排队等待其他请求对齐，**TTFT 和 TPOT 会变差（慢）**。
    *   *比喻：大巴车运一车人，运力强，但每个人都要等车坐满才发车，且路上开得慢。*

> **优化的终极目标**：在满足延迟约束（如 TPOT < 50ms）的前提下，尽可能提高吞吐量（Batch Size 尽可能大）。

---

### **4. 案例分析：不同场景怎么看指标？**

| 场景 | 关键指标 | 为什么？ |
| :--- | :--- | :--- |
| **实时聊天 (Chatbot)** | **TTFT** & **TPOT** | 用户需要立刻看到反馈，且生成的字要跟上阅读速度。吞吐量次要。 |
| **代码补全 (Copilot)** | **Total Latency** | 只有补全完整了才有意义。必须在用户敲下一个键之前完成，要求极低延迟。 |
| **离线摘要/数据分析** | **Throughput (TPS)** | 用户把文件丢进去，过10分钟来看结果也可以。此时要让显卡满负荷运转，处理越多越好。 |

---

### **本节考核点 (Self-Check)**

1.  **公式题**：如果 TTFT 是 0.5秒，生成了 100 个 Token，总共花了 2.5秒。请问 TPOT 是多少？
    *   *计算*：$(2.5 - 0.5) / 100 = 0.02秒 = 20ms$。
2.  **场景题**：老板让你优化一个用来做“文章摘要生成”的后台服务（用户不用实时看），你应该优先优化 Latency 还是 Throughput？
    *   *答案*：Throughput。因为是后台离线任务，不需要实时交互，省钱（提高硬件利用率）更重要。
3.  **概念题**：增加 Batch Size 通常会让 TPOT 变大还是变小？
    *   *答案*：变大（变慢）。因为计算量增加了，虽然并行了，但由于内存带宽竞争和对齐等待，单个请求的生成通常会微变慢，但整体系统的 TPS 会大幅增加。

---

**下一节预告**：
基础理论的“内功心法”我们已经学完了！
接下来我们要进入 **第二阶段：核心技术**。我们将直面优化路上的第一个“大Boss” —— **显存瓶颈**。
我们将深入学习 **KV Cache 的显存计算** 以及 **量化技术 (Quantization)**。

**准备好进入硬核技术环节了吗？**
