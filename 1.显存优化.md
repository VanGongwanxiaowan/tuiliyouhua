# 7.1显存优化
- 提高吞吐量
- 延迟
- qunatized KV Cache
  量化应用在kv cache上的，
- MQA/GPA
  成本最低，减少kvcache的数量

  <img width="269" height="140" alt="image" src="https://github.com/user-attachments/assets/ce9ec47a-b7f6-4326-9780-08dd4b38b7f0" />

<img width="542" height="85" alt="image" src="https://github.com/user-attachments/assets/476d3cd1-6663-4eb9-bd75-73c3ce559ae4" />

- FlashAttentiom
- PagedAttention
  - KV cache 特点：
      - 显存占用大，14b 级别模型，每个 token 需要约 0.7M - 1M 的显存。
      - 动态变化：KV 缓存的大小取决于序列长度，高度可变且不可预测，有效管理挑战大。现有系统因碎片化和过度保留浪费了 60% - 80% 的显存。
  - PagedAttention：
      - 是受操作系统中虚拟内存和分页经典思想启发的注意力算法，允许在非连续的内存空间中存储连续的 key 和 value。
      - 具体做法：将每个序列的 KV cache 划分为块，每个块包含固定数量 token 的键和值。在注意力计算期间，PagedAttention 内核可有效识别和获取这些块。块在内存中无需连续，可像操作系统虚拟内存一样灵活管理 key 和 value，将块视为页面，token 视为字节，序列视为进程，序列的连续逻辑块通过块表映射到非连续物理块，物理块在生成新 token 时按需分配。
      - 内存浪费情况：仅发生在序列的最后一个块，实践中可实现接近最佳的内存使用，仅浪费不到 4%。
      - 关键优势 - 高效的内存共享：
          - 场景：并行采样中，多个输出序列由同一个 prompt 生成时，prompt 的计算和内存可在输出序列中共享。
          - 实现方式：PagedAttention 通过块表启动内存共享，不同序列通过将逻辑块映射到同一个物理块的方式共享块，对物理块的引用计数进行跟踪，并实现写时复制（Copy-on-Write）机制。
          - 效果：内存共享大大减少了复杂采样算法的内存开销，例如并行采样和集束搜索的内存使用量降低了 55%，可转化为高达 2.2 倍的吞吐量提升。
  - vllm的其他优化：continuous batching、CUDA kernel优化
  
- FlashAttention2
- Flash-Decoding
- SparQ Attention
# 7.2 算子融合
# 7.3 高性能算子
# 7.4 调度优化
# 7.5 量化
# 7.6 模型并行
# 7.7 请求优化
# 7.8 采样/解码
- speculative decoding = draft model + reject sampling
- Blockwise Parallel Decoding = multi-draft model + parallel verification
- Medusa = decoding head + tree attention + typical acceptance(threshold)
- SoT = Prompt-based Parallel Decoding
- SpecInfer = Speculative decoding + token tree verification
- LLMA = reuse + parallel verification
- EAGLE = word embedding + simplified tree verification + multi-run speculative sampling
- Lookahead Decoding = n-gram + Jacobi iteration + parallel verification

# GPU关联知识

# GPU关联知识
## 一、GPU的发展历程
- 架构演变：Fahrenheit 架构（1998年）-> Celsius 架构（1999年）-> Kelvin 架构（2001年）-> Rankine 架构（2003年）-> Curie 架构（2004年）
- 各代GPU型号及相关信息（表格形式，包含年份、算力、架构等）：
    - Tesla（2006年）：算力1.0 - 1.5，架构有G80、G84、G86、G92、GT200等
    - Fermi（2010年）：算力2.0 - 2.1，架构GF100、GF110、GF104、GF106、GF108、GF114、GF116
    - Kepler（2012年）：算力3.0 - 3.7，架构GK104、GK106、GK110
    - Maxwell（2014年）：算力5.0 - 5.3，架构GM200、GM204、GM206
    - Pascal（2016年）：算力6.0 - 6.2，架构GP100、GP102、GP104、GP106、GP107
    - Volta（2017年）：算力7.0 - 7.5，架构GV100
    - Turing（2018年）：算力7.0 - 7.5，架构TU102、TU104、TU106、TU116、TU117
    - Ampere（2020年）：算力8.0 - 8.7，架构GA100、GA102、GA104、GA106、GA107
    - Hopper（2022年）：算力9.0 - 9.1，架构GH100
    - Lovelace（2022年）：算力8.0 - 8.9，架构AD102、AD104、AD106、AD107

## 二、GPU基本数据
（图中未明确展开，属于GPU关联知识分支之一）

## 三、GPU任务管理
### 一、CUDA并行程序的线程单元
- **thread**：是CUDA并行程序最基本的执行单元（the basic unit of execution）。
- **warp**：通常包含32个thread，每个warp中的thread可同时执行相同指令，实现SIMT（单指令多线程）并行。warp是SM中最小的调度单位（the smallest scheduling unit on an SM），一个SM可同时处理多个warp。
- **thread block**：可包含多个warp，同一个block中的thread可同步，也可通过shared memory通信。它是GPU执行的最小单位（the smallest unit of execution on the GPU）。若block所含thread数量不是warp大小的整数倍，多出的warp中会有inactive的thread，这些thread会消耗SM资源。
- **grid**：是由多个thread block组成的二维或三维数组，其大小取决于计算任务规模和thread block大小，需结合计算任务特点和GPU性能调整。

### 二、CUDA视角与硬件视角的对应关系（表格）
| CUDA视角 | 功能 | 硬件视角 |
| ---- | ---- | ---- |
| thread | 最小的计算单元，每个thread拥有自己的程序计数器和状态寄存器 | 对应于Core, or lanes |
| warp | 最小的执行调度单元，一个SM的CUDA Core会被分成几个warp | Warp Scheduler一次调度一个warp |
| block | 一个block中的warp只能在同一个SM上调度 | 对应于SM，一个warp中的thread必然在同一个block |
| grid | 一个GPU | 对应于GPU |

### 三、线程管理（基于SM的SIMT架构）
- SM采用Single-Instruction Multiple-Thread（SIMT，单指令多线程）架构，warp是最基本的执行单元，一个warp包含32个并行thread，这些thread以不同数据资源执行相同指令。
- 当kernel被执行时，grid中的thread block被分配到SM上，大量thread可能被分到不同SM，但一个线程块的thread只能在一个SM上调度，SM一般可调度多个block。
- 每个thread拥有自己的程序计数器和状态寄存器，可使用不同数据执行指令，实现并行计算。一个CUDA core可执行一个thread，一个SM中的CUDA core会被分成几个warp，由warp scheduler负责调度。
- GPU规定warp中所有thread在同一周期执行相同指令，尽管执行同一程序地址，但可能因分支结构产生不同行为。一个SM同时并发的warp有限，SM需为每个block分配共享内存，为每个warp中的thread分配独立寄存器，因此SM的配置会影响其支持的block和warp并发数量。
- 

## 四、GPU的内存管理
（图中未明确展开，属于GPU关联知识分支之一）

## 五、GPU架构
（图中未明确展开，属于GPU关联知识分支之一）

## 六、Tensor Core历史
- **第一代（NVIDIA Volta架构，2017年5月发布）**：
    - 是新型处理核心，执行专门的矩阵数学运算，适用于深度学习和某些类型的HPC。
    - 每个Tensor Core可执行64次融合乘法加法（FMA，两个4×4 FP16矩阵相乘，结果添加到4×4 FP16或FP32矩阵中，输出新的4×4 FP16或FP32矩阵）。
    - 一个SM的所有8个Tensor core每时钟共执行512次FMA或1024次单个浮点运算，Tesla V100 Tensor Core可为训练和推理提供125 Tensor TFLOPS。
- **第二代（Turing架构）**：
    - 支持精度FP16、INT8、INT4、INT1。
    - 每个张量核心可执行64个浮点融合乘法加法（FMA）操作，每个时钟使用FP16输入。一个SM中的八个张量核心每时钟执行512个FP16乘法和累加运算，或每个时钟总共执行1024次浮点运算。
    - 新的INT8精度模式以两倍的速率工作，即每时钟2048次整数运算。可支持112 TFLOPS FP16，228 TOPS INT8，455 TOPS INT4。
- **第三代（Ampere架构）**：
    - 支持精度FP64、TF32、bfloat16、FP16、INT8、INT4、INT1。
    - 每个TensorCore在每个时钟周期支持的混合精度矩阵乘加从Volta的4×4×4进化到8×4×8。
    - 附表格《Comparison of NVIDIA Turing vs Ampere Architecture Tensor Core》，对比了TUI202 SM（RTX 2080 Super）、GA1020 SM（RTX 3090）、GA100 SM（A100）的GPU架构、Tensor Cores per SM、FP16 FMA operations per Tensor Core、Total FP16 FMA Operations per SM等参数。
- **第四代**：
    - 支持精度FP64、TF32、bfloat16、FP16、FP8、INT8。
    - 与上一代16位浮点计算相比，Tensor Core在同等数据类型上计算速度是A100 SM中MMA的2倍，在使用FP8数据类型时，计算速度是A100的34倍。每个TensorCore在每个时钟周期支持的混合精度矩阵乘加进化到4×8×16。
    - 附表格对比A100 SM和H100 SM在FP8、TF32、FP16、INT8、FP64等格式下的性能，H100 SM在多个格式下性能是A100 SM的2倍，且支持新的FP8格式，在数据管理上更高效，还能加速稀疏张量运算。
 
  # gpu架构
  ### 一、AD102架构组成
- 拥有12个GPC（图形处理簇，每个GPC接近一枚完整图形流水线的小GPU，GA102有8组）。
- 每个GPC包含6个TPC（纹理处理簇）、一个Raster Engine；每个TPC包含两个SM（流式多处理器），合计144个SM。
- 每个SM内包含4组各32个CUDA Core（按OpenCL术语是PE的SubCore），因此完整的AD102 GPU共18432个CUDA Core。
- GeForce RTX 4090因禁用一个GPC，CUDA Core数量为16834个。

### 二、光栅引擎工作流程
光栅引擎由三个阶段组成：
- **Edge Setup（边缘设置）阶段**：获取顶点位置并计算三角形边缘方程，用于判断像素是否在三角形内；不面向视锥体的三角形通过背面剔除（Back Culling）移除；每个边缘设置单元每个时钟周期最多处理一个点、线或三角形。
- **Rasterizer（光栅化器）阶段**：为每个图元运行边缘方程并计算像素覆盖；若开启抗锯齿功能，会为每个多采样及覆盖采样执行重置操作；每个光栅化器每个时钟周期输出8个像素，整个芯片每个时钟周期内共有32个光栅化完成的像素。
- **Z-Cull（Z剔除）阶段**：光栅化器产生的像素被发送到该单元；Z-Cull单元获取一个像素tile（以tile为单元）进行测试，将tile中的像素深度与帧缓冲区中的现有像素tile比较；完全位于帧缓冲区像素后面的像素tile从管道中剔除，避免后续不必要的像素着色工作（类似Early-Z，用于提早剔除不必要图元，避免OverDraw）。

  <img width="490" height="519" alt="image" src="https://github.com/user-attachments/assets/2c32900a-c27c-4eac-8e10-66c302212188" />



## A100

<img width="853" height="419" alt="image" src="https://github.com/user-attachments/assets/184830d8-97bd-49a2-b75a-0e190f205820" />

<img width="659" height="555" alt="image" src="https://github.com/user-attachments/assets/a21fe32c-4af2-45c4-8d58-1e33c22194d7" />


### 一、4090架构（SM单元细节）
- 每个SM包含4个处理单元，共用**L1 Instruction Cache（一级指令缓存）**、**L1 Data Cache（一级数据缓存/共享内存）**，以及4个内含texture cache的处理单元。
- 每个处理单元包含：
  - 1个Warp Scheduler（每个Warp最多同时执行32个thread）
  - 1个Dispatch Unit
  - 8个FP64 Core、16个FP32 Core、16个INT32 Core
  - 1个Tensor Core
  - 8个Load/Store units（LD/ST Unit）
  - 4个Special Function Units（SFU，用于运算超越函数如sin、cos、exp、log等）
  - 寄存器文件、L0指令缓存
- Tensor Core硬件结构专为矩阵批量运算设计，效率提升十几倍，支持运算类型包括FP16、BF16、TF32、FP64、INT4、INT8。其中，FP16单元是半精度单元，用于张量积加速深度学习操作；图灵Tensor Core还支持INT8和INT4精度操作，适用于可接受量化且无需FP16精度的场景。

### 二、GA100架构
- 整体结构：整个GPU有8个GPC（图形处理集群），单个GPC包含8组TPC，每个TPC包含2个SM（流式多处理器），GPC可视为独立GPU。SM是GPU核心计算单元，GPU并行性由SM决定。大量SM通过L2 Cache、显存和全局调度器（GigaThread Engine）连接，再配合与外部通信的线路，构成数据中心的GA100架构显示核心。
- 具体参数：
  - 8 GPCs，8 TPCs/GPC，2 SMs/TPC，16 SMs/GPC，128 SMs per full GPU
  - 64 FP32 CUDA Cores/SM，8192 FP32 CUDA Cores per full GPU
  - 4 third-generation Tensor Cores/SM，512 third-generation Tensor Cores per full GPU
  - 6 HBM2 stacks，12 512-bit memory controllers

### 三、纹理单元（Texture）
- 功能：是针对GPU图像应用场景设计的存储处理单元，操作输入一般为纹理坐标，输出可为像RGBA这样的颜色坐标，完成坐标到过滤后样本的转换。
- 优势：CPU存储数据线性连续，读取小于单位数量（如64B）的数据时会浪费带宽（如读取4B局部图像数据会读入60B无用信息），而texture单元支持2维（现支持3D多维）坐标拿取并处理数据，设计为只读属性且操作顺序相对严格，因此带宽开销更小、时延更低，但操作灵活度稍小。
- 硬件配套：texture配有地址生成器单元（AGUs）和过滤器单元（filter units），每个SM还配备专门的纹理缓存。

## GPU线程管理

SM采用**Single-Instruction Multiple-Thread（SIMT，单指令多线程）**架构，相关线程管理内容如下：
- **warp**：是最基本的执行单元，一个warp包含32个并行thread，这些thread以不同数据资源执行相同的指令。
- **thread block与SM调度**：当一个kernel被执行时，grid中的thread block被分配到SM上，大量的thread可能被分到不同的SM上，但一个线程块的thread只能在一个SM上调度，SM一般可以调度多个block。
- **thread特性**：每个thread拥有自己的程序计数器和状态寄存器，且可以使用不同的数据来执行指令，从而实现并行计算，即Single Instruction Multiple Thread。
- **CUDA core与warp的关系**：一个CUDA core可以执行一个thread，一个SM中的CUDA core会被分成几个warp，由warp scheduler负责调度。
- **warp执行与SM资源限制**：GPU规定warp中所有thread在同一周期执行相同的指令，尽管这些thread执行同一程序地址，但可能因分支结构产生不同行为。一个SM同时并发的warp是有限的，由于资源限制，SM要为每个block分配共享内存，也要为每个warp中的thread分配独立的寄存器，所以SM的配置会影响其所支持的block和warp并发数量。

# GPU内存管理

<img width="642" height="401" alt="image" src="https://github.com/user-attachments/assets/c49bc75a-5ace-42da-8c54-114526751694" />

### GPU内存管理相关内容
- **内存硬件分类（按是否在芯片上）**
  - **片上（on chip）内存**：主要用于缓存（cache）以及少量特殊存储单元（如texture），特点是速度快、存储空间小。
  - **片下（off chip）内存**：主要用于全局存储（global memory）即常说的显存，特点是速度相对慢、存储空间大。不同于CPU系统内存可扩展的设计，GPU的整体内存大小是固定的，选好显卡型号后就确定，包括缓存和全局存储。高带宽存储HBM（High Bandwidth Memory）是常用的off-chip GPU存储硬件，由多个DDR芯片堆叠后和GPU封装在一起，实现大容量、高位宽的DDR组合阵列。
- **内存功能细分**
  GPU内存可分为：局部内存（local memory）、全局内存（global memory）、常量内存（constant memory）、共享内存（shared memory）、寄存器（register）、L1/L2缓存等。
- **RAM类型**
  - 静态RAM（SRAM）：存入数据后，即使不刷新也不会丢失记忆。
  - 动态RAM（DRAM）：电容需要周期性地充电，否则无法确保记忆长存；密度高、成本低、访问速度较慢、耗电量大。
  - 应用场景：SRAM因速度快、功耗低，适用于带宽要求高或功耗要求低的情境，如CPU Cache、GPU On-Chip Buffer；DRAM一般用于系统内存、显存。GPU片上内存都是SRAM。
- **内存层级结构（图示）**
  由多个SM（流式多处理器）组成，每个SM配备L1缓存，多个L1缓存连接到L2缓存，L2缓存再连接到DRAM（动态随机存取存储器）。

<img width="707" height="617" alt="image" src="https://github.com/user-attachments/assets/b874eb70-e8fd-40f3-afff-56b1680a5911" />
### GPU内存（显存）的理解与基本使用
- **L2缓存**：
  - 可被所有SM访问，速度比全局内存快；合理运用能提速运算。A100的L2缓存可设置至多40MB的持续化数据（persistent data），能拉升算子kernel的带宽和性能。Flash attention的思路是尽可能利用L2缓存，减少HBM的数据读写时间。
- **L1缓存**：
  - 用于存储SM内的数据，被SM内的CUDA cores共享，但跨SM之间的L1不能相互访问。
- **寄存器（register）**：
  - 是线程能独立访问的片上（on chip）存储资源，用来存储线程的暂存数据。速度是访问中最快的，但容量较小，只有几百甚至几十KB，且要被许多线程均分。
- **共享内存（shared memory）**：
  - 是在线程块内能访问的片上（on chip）存储，访问速度较快。主要用于缓存需要反复读写的数据。与L1缓存位置、速度极其类似，区别在于共享内存的控制与生命周期管理由用户控制，L1受系统控制；共享内存更利于线程块之间数据交互。
- **图像/纹理（texture memory）**：
  - 是针对图形化数据的专用内存，texture实际是指通常理解的1D/2D/3D结构数据，相邻数据之间存在一定关系或需要进行相同运算。由global + cache + 处理单元构成，为只读内存。
- **内存分类维度**：
  - 按是否在芯片上分为on-chip/SRAM（片上，如L1、L2、寄存器、共享内存等）和off-chip/DRAM（片下，如全局内存等）。
 

### L2 缓存
- 可被所有 SM 访问，速度比全局内存快；合理运用能够提速运算。
- A100 的 L2 缓存能够设置至多 40MB 的持续化数据（persistent data），可拉升算子 kernel 的带宽和性能。
- Flash attention 的思路是尽可能利用 L2 缓存，减少 HBM 的数据读写时间。

### L1 缓存
- 用于存储 SM 内的数据，被 SM 内的 CUDA cores 共享，但跨 SM 之间的 L1 不能相互访问。

### 寄存器（register）
- 是线程能独立访问的片上（on chip）存储资源，用于存储线程的暂存数据。
- 速度是访问中最快的，但容量较小，只有几百甚至几十 KB，且要被许多线程均分。

### 共享内存（shared memory）
- 是在线程块内能访问的片上（on chip）存储，访问速度较快。
- 主要用于缓存需要反复读写的数据。与 L1 缓存位置、速度极其类似，区别在于共享内存的控制与生命周期管理由用户控制，L1 受系统控制；共享内存更利于线程块之间数据交互。

### 图像/纹理（texture memory）
- 是针对图形化数据的专用内存，实际指通常理解的 1D/2D/3D 结构数据，相邻数据之间存在一定关系或需要进行相同运算。
- 由 global + cache + 处理单元构成，为只读内存。

### 内存分类维度
- 按是否在芯片上分为 on-chip/SRAM（片上，如 L1、L2、寄存器、共享内存等）和 off-chip/DRAM（片下，如全局内存等）。

  <img width="406" height="607" alt="image" src="https://github.com/user-attachments/assets/fe20baef-eae0-4f7f-9c32-9f08a70f0037" />

## FlashAttention

### 计算性能与带宽的关系（两种解释）
- **解释1**：
  记模型计算强度单位为FLOP/byte（代表模型进行单位byte数据交互可实现的操作数），带宽β即模型计算性能，单位为FLOP/s。令L_max=计算平台算力/计算平台带宽，当模型计算强度小于平台理论计算强度L_max，模型计算能力P即β；当模型计算强度大于L_max，模型计算性能P等于平台算力。若模型计算强度小，瓶颈在带宽；若计算强度大，瓶颈在算力。提高计算性能需提高计算强度，即每bytes数据交互的操作数。
- **解释2**：
  记N=每次操作要求的FLOP，单位FLOP/OP；pi=平台算力，单位FLOP/s；beta=内存带宽，单位byte/s；P=实际实现计算速度，单位为FLOP/s；优化目标为O=P/N（每秒钟实现的运算次数，单位为OP/s）。由于N固定，优化目标转为P，P=min(beta, r*L_max=beta*r, pi/beta, pi)，故优化目标转为beta，即改变内存访问策略，实现beta最大化。

### FlashAttention 相关内容
- **出发点**：注意力操作中，S和P的计算空间复杂度都是O(N^2)，此外scale、mask、softmax、dropout都是带宽约束操作。注意力公式为：$\text{Attention}(Q,K,V) = \text{softmax}\left( \frac{QK^T}{\sqrt{d_k}} \right) V$，其中$Q,K,V \in \mathbb{R}^{N \times d}$（$N$表示序列长度，$d$表示维度），可拆解为：$S = QK^T \in \mathbb{R}^{N \times N}$，$P = \text{softmax}(S) \in \mathbb{R}^{N \times N}$，$O = PV \in \mathbb{R}^{N \times d}$。
- **思路**：输入数据K、Q、V存储在HBM上，中间结果S、A都不需要存储到HBM上。通过这种方式，FlashAttention可将内存开销降低到线性级别，并实现了2-4倍的加速，同时避免了对中间结果的频繁读写，从而提高了计算效率。
- **方法**：O(N^2)空间复杂度的矩阵计算对HBM的读写是主要内存瓶颈，主要优化点：
  1. 在不访问整个输入的情况下计算softmax；
  2. 不为反向传播存储大的中间attention矩阵。
  FlashAttention提出两种方法来分步解决上述问题：
  - **tiling**：注意力计算被重新构造，将输入分割成块，并通过在输入块上进行多次传递来递增地执行softmax操作。
  - **recomputation**：存储来自前向的softmax归一化因子，以便在反向传播重新计算芯片上的attention，这比从HBM读取中间矩阵的标准注意力方法更快。虽导致FLOPS增加，但大量减少HBM访问，运行速度更快。其核心思想是分割输入，将其从慢速HBM加载到快速SRAM，然后计算这些块的attention输出，在每个块的输出相加之前按正确的归一化因子缩放，得到正确结果。
  - **kernel融合**：tiling分块计算使得可用一个CUDA kernel来执行注意力的所有操作，从HBM中加载输入数据，在SRAM中执行所有计算操作（矩阵乘法、mask、softmax、dropout、矩阵乘法），再将计算结果写回到HBM。
 
<img width="479" height="642" alt="image" src="https://github.com/user-attachments/assets/5111507b-6ddf-48d2-bc5d-9a50ca65be1d" />


<img width="493" height="236" alt="image" src="https://github.com/user-attachments/assets/8fa8b662-e8cb-4724-85d8-bfe947e99444" />

### FlashAttentionv2 相关内容
- **出发点**：FlashAttention利用GPU内存的非对称层次结构，将内存消耗降至线性（而非二次方），较优化基线实现2到4倍运行速度提升，但速度仍未达优化矩阵乘法（GEMM）操作的速度。前向传播计算吞吐量仅达理论最大浮点运算速率（FLOPs/s）的30-50%，反向传播仅达25-35%。低效率源于GPU上不同线程块间负载分配不佳，导致低占用率或不必要的共享内存读/写。
- **方法**：
  - (1) 调整算法，减少中间缩放次数。通常实现Softmax算子为数值稳定性会减去最大值，需对token遍历3次；若不保存中间最大值和指数和，只保留对数指数和，可减少非矩阵乘法操作的浮点计算次数。
  - (2) 序列长度维度的并行。v1在batch_size和num_heads维度使用并行，但处理长序列输入时因内存限制会减小batch_size和head数量，并行化程度降低；FlashAttention-v2在序列长度这一维度上进行并行化，显著提升计算速度。序列并行要求交换循环层次，将Q移到外循环，K移到内循环，使分块计算的注意力值矩阵块只存储在SRAM上，避免HBM上的频繁读写（$b^{*}s^{*}d->b^{*}s^{*}k^{*}m$）。
  - (3) 分散线程块内部warps之间的工作负载，减少共享内存通信。v1使用split-k策略，所有warp将中间结果写入共享内存同步后相加，共享内存读取拖慢前向传播计算；v2使用split-Q策略，每个warp计算$QK^{T}$后，结果只需对应V分片即可得到O的对应分片，减少中间共享内存读写。

<img width="220" height="83" alt="image" src="https://github.com/user-attachments/assets/69d1175e-4049-4e0e-b19a-67b71e068a03" />



- **结果**：前向传播可达到理论最大吞吐量的73%，反向传播可达到63%。用于端到端训练GPT风格模型时，每个A100 GPU的训练速度可达225 TFLOPs/s。
- **细节1**：GPU由计算元素（如浮点运算单元）和内存层次结构组成。现代GPU通常包含专门单元加速低精度（如FP16/BF16）矩阵乘法运算，例如NVIDIA GPU上的Tensor Core。内存层次结构包括高带宽存储器（HBM上的存储器属于DRAM）和静态随机存取存储器（GPU上所有on-chip memory都是SRAM）。以A100 GPU为例，拥有40-80GB高带宽存储（HBM），带宽达1.5-2.0TB/s，每个流式多处理器有192KB芯片上SRAM（带宽估计约为19TB/s）。NVIDIA的A100 GPU最大理论吞吐量达312 TFLOPs/s（FP16/BF16格式矩阵乘法性能），但非矩阵乘法操作效率较低。



<img width="727" height="625" alt="image" src="https://github.com/user-attachments/assets/5b96a0a2-cc40-464e-83bf-9d98f1abca05" />

### FlashAttentionv2 相关内容
- **出发点**：FlashAttention利用GPU内存非对称层次结构，将内存消耗降至线性（而非二次方），较优化基线实现2-4倍运行速度提升，但速度未达优化矩阵乘法（GEMM）操作的速度。前向传播计算吞吐量仅达理论最大浮点运算速率（FLOPs/s）的30-50%，反向传播仅达25-35%。低效率源于GPU上不同线程块间负载分配不佳，导致低占用率或不必要的共享内存读/写。
- **方法**：
  - (1) 调整算法，减少中间缩放次数。通常实现Softmax算子为数值稳定性会减去最大值，需对token遍历3次；若不保存中间最大值和指数和，只保留对数指数和，可减少非矩阵乘法操作的浮点计算次数。
  - (2) 序列长度维度的并行。v1在batch_size和num_heads维度使用并行，处理长序列输入时因内存限制会减小batch_size和head数量，并行化程度降低；FlashAttention-v2在序列长度维度并行化，显著提升计算速度。通过交换循环层次，将Q移到外循环，KV移到内循环，使分块计算的注意力值矩阵块只存储在SRAM上，避免HBM上的频繁读写（$b^{*}s^{*}d->b^{*}s^{*}k^{*}m$）。
  - (3) 分散线程块内部warps之间的工作负载，减少共享内存通信。v1使用split-k策略，所有warp将中间结果写入共享内存同步后相加，拖慢前向传播计算；v2使用split-Q策略，每个warp计算$QK^{T}$后，只需对应V分片即可得到O的对应分片，减少中间共享内存读写。
- **结果**：前向传播可达到理论最大吞吐量的73%，反向传播可达到63%。用于端到端训练GPT风格模型时，每个A100 GPU的训练速度可达225 TFLOPs/s。
- **细节1**：GPU由计算元素（如浮点运算单元）和内存层次结构组成。现代GPU通常包含专门单元加速低精度（如FP16/BF16）的矩阵乘法运算，例如Nvidia GPU上的Tensor Core。内存层次结构包括高带宽存储器（HBM上的存储器属于DRAM）和静态随机存取存储器（GPU上所有on-chip memory都是SRAM）。


<img width="658" height="266" alt="image" src="https://github.com/user-attachments/assets/b7dba594-ea84-4f35-805d-10b19f2c8a0f" />


<img width="605" height="736" alt="image" src="https://github.com/user-attachments/assets/b30a2e40-50c9-4924-874f-8b42470e2487" />


### 一、FlashAttentionv2 结果与GPU细节
- **结果**：
  前向传播可达到理论最大吞吐量的73%，反向传播可达到63%。当用于端到端训练GPT风格模型时，每个A100 GPU的训练速度可以达到225 TFLOPs/s。
- **细节1（GPU组成与内存层次）**：
  - GPU由计算元素（如浮点运算单元）和内存层次结构组成。现代GPU通常包含专门单元加速低精度（如FP16/BF16）的矩阵乘法运算，例如Nvidia GPU上的Tensor Core。
  - 内存层次结构包括高带宽存储器（HBM上的存储都属于DRAM）和静态随机存取存储器（GPU上所有的on-chip memory都是SRAM）。以A100 GPU为例，它拥有40-80GB的高带宽存储（HBM），带宽达到1.5-2.0TB/s，每个流式多处理器（共108个）有192KB芯片上SRAM，带宽估计约为19TB/s。
  - Nvidia的A100 GPU最大理论吞吐量：FP16/BF16格式矩阵乘法性能达312 TFLOPs/s（每秒万亿次浮点运算），非矩阵乘法的FP32格式计算仅为19.5 TFLOPs/s。
- **细节2（GPU线程与kernel执行）**：
  - GPU有大量threads用于执行操作（称为kernel）。threads组成thread block，block被调度在SMs上运行。每个thread block中，threads组成warps（32个threads为一组）。
  - 一个warp内的threads可通过快速shuffle指令通信或合作执行矩阵乘法；每个thread block内部，warps可通过读取/写入共享内存通信。
  - 每个kernel从HBM加载数据到寄存器和SRAM中进行计算，最后将结果写回HBM中。

### 二、其他分支
- Flash-Decoding
- SparQ Attention
- 参考文献

  ### Flash-Decoding 相关内容
- **出发点**：FlashAttention优化不适合直接应用于推理过程。因为训练中FlashAttention对batch size和query length并行化加速，而推理时query length通常为1，若batch size小于GPU的SM数量（例如A100有108个SMs），计算过程仅使用GPU一小部分；上下文较长时，常减小batch size适配GPU内存，如batch size = 1时，FlashAttention对GPU利用率小于1%。
- **并行化维度扩展**：FlashAttention对batch size和query length并行化加速，Flash-Decoding在此基础上增加keys/values的序列长度这一并行化维度。即使batch size很小，只要上下文足够长，就能充分利用GPU。且几乎不用额外存储大量数据到全局内存，减少内存开销。
- **主要步骤**：
  1. 将keys和values分成较小的block；
  2. 使用FlashAttention并行计算query与每个block的注意力（这是与FlashAttention的最大区别）。对于每个block的每行（一行对应一个特征维度），额外记录attention values的log-sum-exp（标量值，用于第3步rescale）；
  3. 对所有output blocks进行reduction得到最终output，需用log-sum-exp值重新调整每个块的贡献。第1步数据分块不涉及GPU操作，只需对第2、3步执行单独kernels。虽reduction操作引入额外计算，但总体通过增加并行化方式取得更高效率。

### 其他分支
- SparQ Attention
- 参考文献
<img width="577" height="537" alt="image" src="https://github.com/user-attachments/assets/a3f30568-6e55-4554-a872-472bcfb3f4b8" />
