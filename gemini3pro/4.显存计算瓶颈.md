太好了，让我们直击痛点。**“显存瓶颈”**和**“计算瓶颈”**是大模型推理优化中两个最大的“拦路虎”。搞清楚它们，你就能一眼看穿模型为什么跑得慢，或者为什么跑不起来。

我们将这一节分为两部分来深入剖析。

---

### **第一部分：显存瓶颈 (Memory Bottleneck)**

对于大模型推理，**90% 的情况是被显存卡住的**。
显存不仅决定了你能跑多大的模型，更决定了你能同时服务多少用户（Batch Size）。

显存主要被两样东西吃掉了：**静态的模型权重** 和 **动态的 KV Cache**。

#### **1. 模型权重 (Model Weights) - 这里的“房租”**
不管你有没有请求，只要模型加载进显卡，这部分显存就被占用了，就像固定的房租。

*   **计算公式（估算）**：
    $$ \text{显存占用 (GB)} \approx \text{参数量 (Billion)} \times \text{每参数字节数} $$
    *   **FP16 / BF16 (半精度)**：每个参数 2 Bytes。
    *   **FP32 (单精度)**：每个参数 4 Bytes（推理很少用）。
    *   **INT8 (8位量化)**：每个参数 1 Byte。
    *   **INT4 (4位量化)**：每个参数 0.5 Byte。

*   **举个栗子 (Llama-2-7B)**：
    *   用 **FP16** 加载：$7 \times 2 = 14 \text{ GB}$。
    *   用 **INT4** 加载：$7 \times 0.5 = 3.5 \text{ GB}$。
    *   *看到量化的威力了吗？显存直接砍到 1/4！*

#### **2. KV Cache (Key-Value Cache) - 这里的“客流”**
这是**动态**增长的。每多一个并发用户，或者对话每长一点，它就蹭蹭往上涨。
它是推理优化（尤其是长文本）最大的敌人。

*   **计算公式 (精确)**：
    $$ \text{总KV显存} = 2 \times \text{层数} \times \text{Hidden Size} \times \text{Seq Length} \times \text{Batch Size} \times \text{每参数字节数} $$
    *   *为什么乘 2？* 因为要存 Key 和 Value 两个矩阵。

*   **举个栗子 (Llama-2-7B, FP16)**：
    *   参数：32层 (Layers)，4096 隐藏层维度 (Hidden Size)。
    *   假设：Batch Size = 1 (1个用户)，Sequence Length = 1024 (大概一千字的对话)。
    *   计算：$2 \times 32 \times 4096 \times 1024 \times 1 \times 2 \text{ Bytes} \approx 536 \text{ MB}$。
    *   *看起来还好？*
    *   **炸裂时刻**：如果你想支持 **32k 长文本**，且并发 **64 个用户**：
        $536 \text{ MB} \times 32 (\text{长度倍数}) \times 64 (\text{用户倍数}) \approx \mathbf{1 TB} \text{ !!!}$
    *   *这就是为什么现在的卡（如A100 80G）根本跑不动大并发长文本，必须上 PagedAttention 和 MQA/GQA 的原因。*

#### **3. 激活值 (Activations) - 临时的“草稿纸”**
在计算过程中产生的中间结果。相比权重和 KV Cache，推理时的激活值占用较小，通常不是主要矛盾（但在训练时是主要矛盾）。

---

### **第二部分：计算瓶颈 (Compute Bottleneck)**

如果显存没爆，数据也搬运过来了，接下来就看 GPU 的算力核心（CUDA Cores / Tensor Cores）算得够不够快了。

#### **1. 什么时候遇到计算瓶颈？**
*   **Prefill 阶段**：一次性处理几千个 Token，矩阵乘法规模巨大，此时计算单元满载。
*   **大 Batch Size 的 Decode 阶段**：虽然每次只生一个词，但如果有 1000 个用户同时生，矩阵乘法的维度（[1000, Hidden_Size]）变大了，计算压力也会上来。
*   **超大模型**：像 GPT-4 这种级别的巨无霸，参数量太大，每一层计算量惊人。

#### **2. 衡量指标：算术强度 (Arithmetic Intensity)**
这是判断是“显存瓶颈”还是“计算瓶颈”的金标准。

$$ \text{算术强度} = \frac{\text{计算量 (FLOPs)}}{\text{访存量 (Bytes)}} $$

*   **高算术强度 (Compute-bound)**：读一次数据，能在里面算很久。
    *   *例子*：大矩阵乘法（Prefill）。
    *   *对策*：买更好的卡（H100比A100算得快），或者用更低精度的计算（FP8）。
*   **低算术强度 (Memory-bound)**：读一次数据，算一下就扔了，又要读新的。
    *   *例子*：小 Batch 的 Decode（矩阵向量乘法）。
    *   *对策*：**显存带宽 (Memory Bandwidth)** 是王道。这就是为什么 HBM (高带宽内存) 那么贵。

---

### **总结与直觉建立**

想象你在厨房做菜（推理）：

1.  **显存容量 (VRAM Capacity)** = **厨房台面大小**。
    *   *台面不够（显存爆了）*：连菜板（模型权重）和盘子（KV Cache）都放不下，直接没法做饭（OOM 报错）。
2.  **显存带宽 (Memory Bandwidth)** = **你是去冰箱拿菜的速度**。
    *   *速度太慢（Memory-bound）*：台面虽然够大，但你切完一根葱（计算快），要花 10 秒钟去冰箱拿一颗蒜（访存慢）。大部分时间你在走来走去，手里的刀（算力）是停着的。
    *   *这是 Decode 阶段的常态。*
3.  **计算能力 (FLOPS)** = **你切菜的手速**。
    *   *手速太慢（Compute-bound）*：菜都堆在手边了（访存够快），但你切不过来。
    *   *这是 Prefill 阶段的常态。*

---

### **本节考核点 (Self-Check)**

1.  **估算题**：一个 14B 参数的模型，用 FP16 加载，至少需要多大显存？
    *   *答案*：$14 \times 2 = 28 \text{ GB}$。一张 24G 的 3090/4090 跑不起来，必须量化。
2.  **判断题**：在 Decode 阶段，Batch Size 从 1 增加到 8，通常会显著增加推理延迟吗？
    *   *答案*：不会显著增加（微增）。因为此时是 Memory-bound，瓶颈在搬运权重。搬一次权重顺便算 8 个人的数据，和算 1 个人的数据，时间差不太多。这也就是为什么我们要尽量增大 Batch Size。
3.  **痛点分析**：为什么长文本（如处理一本书）会让显存爆炸？
    *   *答案*：主要是 KV Cache 随长度线性增长，几万个 Token 的 KV Cache 甚至会比模型本身还大。

---

**下一节预告**：
明白了痛点，我们就要开始学“解药”了。
下一节进入 **第二阶段：核心技术**。我们将首先学习目前最主流、最有效的显存优化手段 —— **量化 (Quantization)**。
我们将搞懂 INT8、FP4 到底是怎么把模型变小，同时还能保持“聪明”的。

**准备好开始给模型“瘦身”了吗？**
