# 大模型推理优化策略
## 一、核心优化方向
- **7.1 显存优化**
- **7.2 算子融合**
- **7.3 高性能算子**
- **7.4 调度优化**
- **7.5 量化**
- **7.6 模型并行**
- **7.7 通信优化**
- **7.8 采样/解码**
- **参考资料**
调度优化，减少gpu的等待时间，提高gpu的利用率
## 二、调度优化细分
### 1. Dynamic Batching（Nvidia-triton, request-level）
- **静态batching缺点**：批次大小固定，无法随计算资源负载动态变化，导致GPU资源利用率低。
- **Dynamic Batching机制**：通过维护作业队列在batch维度动态插入新序列；缺点是需对输入数据填充使长度一致，或需暂停系统等待构建更大批次。

### 2. Async Servering
- 多线程异步，流水线，overlap，实现降低时延
- tokernize/detokenize过程在cpu上执行，期间gpu处于空闲状态
### 3. continuous/iterative-level batching
- **思路**：对prompt和生成的token动态分解和融合，保证模型前向传播大小一致，避免长prompt占用过多资源；vLLM在一个前向传递中要么生成令牌，要么处理提示，令牌生成抢占提示处理；Orca在生成过程中以完整长度处理提示。
- **实现**：对长prompt分块，多次前传逐步处理，最后一次与生成的token融合；对短prompt填充以精确满足目标token数量。

### 4. Dynamic SplitFuse/sarathi
- **comment**：FastGen = vllm + Dynamic SplitFuse，其splitfuse原理与sarathi类似，sarathi思想与interleaved 1F1B几乎一致，即将一个batch的多个prompt分割成均匀的chunk，不同prompt的chunk交叠排列，执行流水线并行推理；每个prompt的prefill完成后立即进入生成阶段，后续prompt预填充与生成交叠执行，减少bubble时间。

Dynamic Batching(Nividia-trition,request-level)

通过维护一个作业队列实现在batch维度动态插入新的序列，缺点是需要对输入数据进行填充使得长度一致的，或者需要暂停系统来等到构建较大的批次

静态batching批次大小固定不变，无法随着计算资源负载动态变化，导致gpu资源利用率低

dnn模型只需要正常执行一遍就可以了，transformer的自回归每次只生成一个token，sequence token才会停止的

<img width="562" height="546" alt="image" src="https://github.com/user-attachments/assets/9e2e8743-d58d-49ae-b843-ee89347f9be9" />

<img width="558" height="595" alt="image" src="https://github.com/user-attachments/assets/4d336c5a-d4b5-40ea-a202-07f711b8d0b7" />


<img width="471" height="517" alt="image" src="https://github.com/user-attachments/assets/c9901e33-5de9-48b8-bc85-4da7e2a9ae12" />

<img width="418" height="359" alt="image" src="https://github.com/user-attachments/assets/098b7c55-d5ff-4d94-8b7e-43ef5436a08b" />

### Dynamic SplitFuse/sarathi 相关内容
- **comment**：FastGen = vllm + Dynamic SplitFuse，其splitfuse原理与sarathi十分相像。sarathi的思想与interleaved 1F1B几乎一致，即将一个batch的多个prompt分割成均匀的chunk，不同prompt的chunk交叠排列，然后执行流水线并行推理；每个prompt的prefill完成后立即进入生成阶段，后续的prompt预填充与生成交叠执行，从而减少bubble的时间。
- **图示说明**：
  - 图(a)为Baseline Iteration-level scheduling，展示了传统调度下GPU资源利用情况。
  - 图(b)为SARATHI: Chunked prefill with decode-maximal batching，展示了将请求的prefill部分分割为多个chunk交叠执行，实现预填充与解码阶段交叠，提升资源利用率的过程。
  - 对请求A、B、C、D，每个请求的prefill部分可分割为不同大小的块（如$R_{A_1}, R_{A_2}...$），后续解码token为$R_{A_0}, R_{A_1}...$，通过chunk交叠实现并行。

调度优化

dynamic batching（nvida-trition,request-level)

async servering

continuous/iterative-level batching

# 7.6模型并行

tensor paralellism加速和降现场

tensor-wise parallelism

pipeline paralellism减少显存


<img width="504" height="610" alt="image" src="https://github.com/user-attachments/assets/753c8040-6f3a-49bc-a976-9c606a24bf25" />


<img width="360" height="293" alt="image" src="https://github.com/user-attachments/assets/cc678c5f-365b-4e3b-8a25-2927c49cb463" />

<img width="354" height="103" alt="image" src="https://github.com/user-attachments/assets/92be823e-9c52-4f39-8478-b121e80056b7" />


<img width="1204" height="549" alt="image" src="https://github.com/user-attachments/assets/3d18c5cd-9b76-4eee-908d-9bc276317892" />

<img width="418" height="345" alt="image" src="https://github.com/user-attachments/assets/537ba7d8-0ff5-4f2b-b608-76dc40f81972" />

<img width="759" height="629" alt="image" src="https://github.com/user-attachments/assets/dfa361c8-1c55-4273-9b81-4293534d2463" />


<img width="1106" height="632" alt="image" src="https://github.com/user-attachments/assets/c4315c48-ce62-4b8f-a263-b2c06fd8b6e1" />



<img width="1109" height="634" alt="image" src="https://github.com/user-attachments/assets/9aa4a567-acaf-429a-a505-fbb480c26ffa" />
<img width="503" height="433" alt="image" src="https://github.com/user-attachments/assets/e0391aea-543c-4f0e-ad0e-4e44b1cbb62f" />


cross-entroy计算减少通讯量来的

<img width="417" height="151" alt="image" src="https://github.com/user-attachments/assets/07148e15-fe6c-4983-b545-6c063996e0b0" />

<img width="1326" height="769" alt="image" src="https://github.com/user-attachments/assets/fe33c462-1608-4545-82f0-92456702967d" />

 # 流水线并行

 <img width="1111" height="587" alt="image" src="https://github.com/user-attachments/assets/40deff89-e779-44a7-8503-1111b2b51f10" />

<img width="588" height="782" alt="image" src="https://github.com/user-attachments/assets/044e414f-a7ae-49ac-81d7-28c6aaba2e53" />



### 一、Google GPipe 相关内容
- **思路**：将mini-batch分为若干个micro-batch，计算完一些就传给下个节点，最后同步更新参数；将被训练的层划分为多个阶段，每个阶段包含模型中一组连续的层；使用梯度累积优化内存效率，通过丢弃前向传播和后向传播之间的activation存储来交换内存，在需要activation时重新计算。
- **劣势**：过多流水线刷新导致空闲时间增加；若micro-batch数目m很小，会因重新计算开销和频繁管道刷新降低硬件效率，所以m一般设置较大，需缓存m份activation导致内存增加。原因是每个micro-batch前向计算的中间结果activation需被后向计算使用，即使使用Checkpointing技术，前向计算的activation也需等对应后向计算完成后才能释放。令micro-batch数目为m，pipe阶段为p，则空闲比例为$(p - 1)/(p - 1 + m)$。
- **图示**：展示了Worker 1-4在时间维度上的Forward Pass（蓝色）、Backward Pass（绿色）和Idle（灰色）状态，体现流水线调度下的计算与空闲分布。

### 二、Microsoft 1F1B 相关内容
- **思路**：努力减少每个activation的保存时间，需要每个micro-batch数据尽可能早完成后向计算，让每个activation尽可能早释放。
- **方法**：每个GPU以交替方式执行每个micro-batch的正向和反向过程，使activation的缓存数量只跟阶段（stage）数相关，进一步节省显存。
- **步骤**：
  - 启动阶段（Startup State）：输入的stage先读入足够多micro-batch的数据，保证pipeline在稳定阶段时各设备都有工作处理。
  - 稳定阶段：输出阶段完成第一个小批次前向传播后，对同一个小批次执行后向传播，然后交替执行后续小批次的前向和后向传播；当反向传播过程传播到管道早期阶段时，每个阶段在不同小批次的正向和反向过程之间交替进行。
- **权重问题**：同一个micro-batch在不同stage做同样操作（同样前向或后向传播）时使用的参数版本不一致。例如minibatch 5在worker 1上的前向计算部分，其前向逻辑在minibatch 1的后向计算之后执行，但minibatch 5在worker 2上的前向传播在minibatch 1和minibatch 2的后向计算结束后执行。
- **解决方法**：
  - Weight stashing：为权重维护多个版本，每个active micro-batch对应一个版本，每个stage使用最新版本的权重进行前向计算，处理输入的micro-batch，计算新的梯度后，等到这组micro-batch的后向传播完成后再更新到下一个micro-batch的后向计算。
  - Weight stashing需要在一个管道内维护多个版本的权重参数用于不同小批次的前向和后向传播，但不同阶段更新时间不同，一个阶段的小批次使用模型参数的一致性难以保证。
  - Vertical Sync：让micro-batch进入pipeline前使用输入的micro-batch激活卡的参数，且参数的版本同步会跟随micro-batch影响整个生命周期，在各个阶段使用同一个版本的参数，而非Weight stashing那样部分阶段使用较旧的参数，从而实现stage间的参数一致性。
- **图示**：展示了Machine 1-4在Startup State和Steady State下的Forward Work（蓝色）、Backward Work（绿色）和Idle（灰色）状态，体现1F1B调度下的计算流程。

### 三、PipeDream 相关内容
- **PipeDream-2BW（双缓冲区权重，Double-buffered weights）**：为每个微批次生成一个新的模型版本，但因有连续的后向传播，新的梯度需多次计算才能得到版本，且由于维护了两个版本，主机主存压力有所增加。
- **PipeDream-flush**：在PipeDream-2BW之后实现了全异步的管道刷新策略，在阶段仅保留进行中的1in-flight微批次，维持较低的内存占用，能更好地利用时间和内存。


<img width="1172" height="763" alt="image" src="https://github.com/user-attachments/assets/3cef37e6-1804-4f13-8a1e-94b3abe03428" />


<img width="1197" height="783" alt="image" src="https://github.com/user-attachments/assets/15ea04c8-2897-4ed1-b157-8b8a14866920" />


<img width="743" height="349" alt="image" src="https://github.com/user-attachments/assets/13b502e0-5b95-45e9-9d4f-d7f1cf0e7af1" />

# 通信优化

网络通信

- rpc
- grpc
- http

响应模式

- sse
- 轮询
- 长轮询comet
- websocket
- 


